{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bc1286b-8d80-459b-8663-390451e80dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in /opt/anaconda3/lib/python3.12/site-packages (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image) (1.13.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image) (3.2.1)\n",
      "Requirement already satisfied: pillow>=9.1 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image) (2023.4.12)\n",
      "Requirement already satisfied: packaging>=21 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image) (0.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1055b10b-ac15-49cc-b27a-b52192d82ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU detected. Training will run on CPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# Enable Mixed Precision (if GPU is available)\n",
    "# =========================\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPU detected. Enabling mixed precision training.\")\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "else:\n",
    "    print(\"No GPU detected. Training will run on CPU.\")\n",
    "\n",
    "# =========================\n",
    "# 1. Preprocessing Functions\n",
    "# =========================\n",
    "def load_and_preprocess_video(video_path, target_size=(256, 256), max_frames=500):\n",
    "    print(f\"Loading video: {video_path}\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while cap.isOpened() and len(frames) < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, target_size)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = frame / 127.5 - 1.0  # Normalize to [-1, 1]\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    print(f\"Loaded {len(frames)} frames from {video_path}\")\n",
    "    return np.array(frames)\n",
    "\n",
    "def load_videos_from_folder_subset(folder_path, target_size=(256, 256), max_frames=500, num_videos=10):\n",
    "    all_videos = sorted([f for f in os.listdir(folder_path) if f.endswith('.avi')])\n",
    "    videos = all_videos[:num_videos]\n",
    "    frames = []\n",
    "    for video_file in videos:\n",
    "        video_path = os.path.join(folder_path, video_file)\n",
    "        video_frames = load_and_preprocess_video(video_path, target_size, max_frames)\n",
    "        frames.append(video_frames)\n",
    "    total_frames = np.concatenate(frames, axis=0) if frames else np.array([])\n",
    "    print(f\"Total frames loaded from folder {folder_path}: {total_frames.shape[0] if len(total_frames) > 0 else 0}\")\n",
    "    return total_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14fb850f-3bfa-4d5a-9de0-c2d173691494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2. Residual Block (Simplified)\n",
    "# =========================\n",
    "def residual_block(x, filters, kernel_size=3):\n",
    "    shortcut = x\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)  \n",
    "    x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([shortcut, x])\n",
    "    x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
    "    return x\n",
    "\n",
    "# =========================\n",
    "# 3. Simplified GAN Architectures\n",
    "# =========================\n",
    "class TemporalGAN:\n",
    "    def __init__(self, frame_shape=(256, 256, 3), latent_dim=128):\n",
    "        print(\"Initializing TemporalGAN...\")\n",
    "        self.frame_shape = frame_shape\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator = self.build_discriminator()\n",
    "\n",
    "        # Use a slightly lower learning rate for discriminator to slow its convergence\n",
    "        self.gen_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "        self.disc_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        print(\"TemporalGAN initialized.\")\n",
    "\n",
    "    def build_generator(self):\n",
    "        print(\"Building generator with residual blocks...\")\n",
    "        inputs = layers.Input(shape=(self.latent_dim,))\n",
    "        x = layers.Dense(16 * 16 * 256, use_bias=False)(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
    "        x = layers.Reshape((16, 16, 256))(x)\n",
    "        \n",
    "        # Upsample to 32x32\n",
    "        x = layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
    "        # Residual block at 32x32\n",
    "        x = residual_block(x, 128)\n",
    "        \n",
    "        # Upsample to 64x64\n",
    "        x = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
    "        # Residual block at 64x64\n",
    "        x = residual_block(x, 64)\n",
    "        \n",
    "        # Upsample to 128x128\n",
    "        x = layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
    "        \n",
    "        # Upsample to 256x256\n",
    "        outputs = layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', activation='tanh')(x)\n",
    "        \n",
    "        model = models.Model(inputs, outputs)\n",
    "        print(\"Generator built.\")\n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        print(\"Building simplified discriminator...\")\n",
    "        model = models.Sequential([\n",
    "            layers.InputLayer(shape=self.frame_shape),\n",
    "            layers.GaussianNoise(0.1),\n",
    "            layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'),\n",
    "            layers.LeakyReLU(negative_slope=0.2),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "            layers.LeakyReLU(negative_slope=0.2),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'),\n",
    "            layers.LeakyReLU(negative_slope=0.2),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        print(\"Discriminator built.\")\n",
    "        return model\n",
    "\n",
    "    def generator_loss(self, fake_output):\n",
    "        return self.cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "    def discriminator_loss(self, real_output, fake_output):\n",
    "        real_loss = self.cross_entropy(tf.ones_like(real_output) * 0.9, real_output)\n",
    "        fake_loss = self.cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a40d782-7deb-4441-aeed-a09f27ed1fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4. Create an Optimized Data Pipeline\n",
    "# =========================\n",
    "def create_dataset(frames, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(frames)\n",
    "    dataset = dataset.shuffle(1000).batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30300eb2-91f5-40dd-a14b-3d955c0b9943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training videos from: /Users/omvishal/Desktop/B/train/BenchPress\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c01.avi\n",
      "Loaded 151 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c01.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c03.avi\n",
      "Loaded 104 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c03.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c04.avi\n",
      "Loaded 105 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c04.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c05.avi\n",
      "Loaded 88 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c05.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c06.avi\n",
      "Loaded 111 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c06.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g02_c02.avi\n",
      "Loaded 98 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g02_c02.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g02_c04.avi\n",
      "Loaded 95 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g02_c04.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g02_c05.avi\n",
      "Loaded 73 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g02_c05.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g02_c06.avi\n",
      "Loaded 72 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g02_c06.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g03_c02.avi\n",
      "Loaded 100 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g03_c02.avi\n",
      "Total frames loaded from folder /Users/omvishal/Desktop/B/train/BenchPress: 997\n",
      "Total training frames loaded: 997\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 6. Data Loading and Running Training\n",
    "# =========================\n",
    "# Replace with your actual root folder path\n",
    "root_folder = '/Users/omvishal/Desktop/B'  # <<<--- Change this to your folder path\n",
    "train_videos_folder = os.path.join(root_folder, \"train\", \"BenchPress\")\n",
    "print(f\"Loading training videos from: {train_videos_folder}\")\n",
    "train_frames = load_videos_from_folder_subset(train_videos_folder, target_size=(256, 256), max_frames=500, num_videos=10)\n",
    "print(f\"Total training frames loaded: {train_frames.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b96ee19-2ff1-4823-a344-1df6d6a1bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters.\n",
    "epochs = 50\n",
    "batch_size = 8\n",
    "latent_dim = 128\n",
    "output_dir = \"synthetic_frames_test7_50epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "853afd77-c99d-49f5-9c86-1264178036fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of /Users/omvishal/Desktop/B/val/BenchPress → ['v_BenchPress_g12_c06.avi', 'v_BenchPress_g02_c07.avi', 'v_BenchPress_g03_c01.avi', 'v_BenchPress_g02_c01.avi', 'v_BenchPress_g12_c03.avi', 'v_BenchPress_g19_c03.avi', 'v_BenchPress_g09_c03.avi', 'v_BenchPress_g18_c06.avi', 'v_BenchPress_g09_c05.avi', 'v_BenchPress_g19_c06.avi', 'v_BenchPress_g09_c06.avi', 'v_BenchPress_g22_c03.avi', 'v_BenchPress_g05_c05.avi', 'v_BenchPress_g22_c05.avi', 'v_BenchPress_g15_c01.avi', 'v_BenchPress_g04_c05.avi', 'v_BenchPress_g20_c05.avi', 'v_BenchPress_g20_c07.avi', 'v_BenchPress_g17_c05.avi', 'v_BenchPress_g07_c05.avi']\n"
     ]
    }
   ],
   "source": [
    "val_dir = os.path.join(root_folder, \"val\", \"BenchPress\")\n",
    "print(\"Contents of\", val_dir, \"→\", os.listdir(val_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f98a722-2287-4b3b-829d-9e27e9a46f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading video: /Users/omvishal/Desktop/B/val/BenchPress/v_BenchPress_g02_c01.avi\n",
      "Loaded 16 frames from /Users/omvishal/Desktop/B/val/BenchPress/v_BenchPress_g02_c01.avi\n",
      "Total frames loaded from folder /Users/omvishal/Desktop/B/val/BenchPress: 16\n"
     ]
    }
   ],
   "source": [
    "# ─── Replace your val_frames line with this ───\n",
    "val_folder = os.path.join(root_folder, \"val\", \"BenchPress\")\n",
    "# loads up to 16 frames from the first video it finds\n",
    "val_frames = load_videos_from_folder_subset(\n",
    "    val_folder,\n",
    "    target_size=(256,256),\n",
    "    max_frames=16,\n",
    "    num_videos=1\n",
    ")\n",
    "if val_frames.size == 0:\n",
    "    raise RuntimeError(f\"No frames loaded from validation folder: {val_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae18c3c-d29c-443c-80a6-5ff63f0ec603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU detected; running on CPU.\n",
      "Contents of validation folder: ['v_BenchPress_g12_c06.avi', 'v_BenchPress_g02_c07.avi', 'v_BenchPress_g03_c01.avi', 'v_BenchPress_g02_c01.avi', 'v_BenchPress_g12_c03.avi', 'v_BenchPress_g19_c03.avi', 'v_BenchPress_g09_c03.avi', 'v_BenchPress_g18_c06.avi', 'v_BenchPress_g09_c05.avi', 'v_BenchPress_g19_c06.avi', 'v_BenchPress_g09_c06.avi', 'v_BenchPress_g22_c03.avi', 'v_BenchPress_g05_c05.avi', 'v_BenchPress_g22_c05.avi', 'v_BenchPress_g15_c01.avi', 'v_BenchPress_g04_c05.avi', 'v_BenchPress_g20_c05.avi', 'v_BenchPress_g20_c07.avi', 'v_BenchPress_g17_c05.avi', 'v_BenchPress_g07_c05.avi']\n",
      "Loading video: /Users/omvishal/Desktop/B/val/BenchPress/v_BenchPress_g02_c01.avi\n",
      "Loaded 16 frames from /Users/omvishal/Desktop/B/val/BenchPress/v_BenchPress_g02_c01.avi\n",
      "Total frames loaded from folder /Users/omvishal/Desktop/B/val/BenchPress: 16\n",
      "Validation frames shape: (16, 256, 256, 3)\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c01.avi\n",
      "Loaded 151 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c01.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c03.avi\n",
      "Loaded 104 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c03.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c04.avi\n",
      "Loaded 105 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c04.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c05.avi\n",
      "Loaded 88 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c05.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c06.avi\n",
      "Loaded 111 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g01_c06.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g02_c02.avi\n",
      "Loaded 98 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g02_c02.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g02_c04.avi\n",
      "Loaded 95 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g02_c04.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g02_c05.avi\n",
      "Loaded 73 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g02_c05.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g02_c06.avi\n",
      "Loaded 72 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g02_c06.avi\n",
      "Loading video: /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g03_c02.avi\n",
      "Loaded 100 frames from /Users/omvishal/Desktop/B/train/BenchPress/v_BenchPress_g03_c02.avi\n",
      "Total frames loaded from folder /Users/omvishal/Desktop/B/train/BenchPress: 997\n",
      "Training frames shape: (997, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "# ===== GPU + Mixed Precision Setup  =====\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"GPU + mixed precision enabled.\")\n",
    "else:\n",
    "    print(\"No GPU detected; running on CPU.\")\n",
    "\n",
    "\n",
    "# ===== 0. Validate & Load Validation Frames =====\n",
    "root_folder = '/Users/omvishal/Desktop/B'\n",
    "val_folder = os.path.join(root_folder, \"val\", \"BenchPress\")\n",
    "\n",
    "print(\"Contents of validation folder:\", os.listdir(val_folder))\n",
    "\n",
    "val_frames = load_videos_from_folder_subset(\n",
    "    val_folder,\n",
    "    target_size=(256,256),\n",
    "    max_frames=16,\n",
    "    num_videos=1\n",
    ")\n",
    "if val_frames.size == 0:\n",
    "    raise RuntimeError(f\"No frames loaded from validation folder: {val_folder}\")\n",
    "# ensure exactly 16 for consistency\n",
    "val_frames = val_frames[:16]\n",
    "print(f\"Validation frames shape: {val_frames.shape}\")  # should be (16,256,256,3)\n",
    "\n",
    "# ===== Prepare your training frames as before =====\n",
    "train_videos_folder = os.path.join(root_folder, \"train\", \"BenchPress\")\n",
    "train_frames = load_videos_from_folder_subset(\n",
    "    train_videos_folder,\n",
    "    target_size=(256,256),\n",
    "    max_frames=500,\n",
    "    num_videos=10\n",
    ")\n",
    "print(f\"Training frames shape: {train_frames.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4588e1a-54aa-49b0-978a-f1c01aa72990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "def train_temporal_gan_with_visuals(\n",
    "    frames,\n",
    "    val_frames,\n",
    "    epochs=50,\n",
    "    batch_size=8,\n",
    "    latent_dim=128,\n",
    "    output_dir=\"synthetic_frames_with_visuals\",\n",
    "    val_interval=1,\n",
    "    sample_interval=10\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    gen_losses, disc_losses = [], []\n",
    "    psnr_scores, ssim_scores = [], []\n",
    "    \n",
    "    dataset = create_dataset(frames, batch_size)\n",
    "    tgan = TemporalGAN(frame_shape=(256,256,3), latent_dim=latent_dim)\n",
    "    steps_per_epoch = len(frames) // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        batch_g, batch_d = [], []\n",
    "        for step, real in enumerate(dataset):\n",
    "            if step >= steps_per_epoch:\n",
    "                break\n",
    "            \n",
    "            # Generator updates (×2)\n",
    "            for _ in range(2):\n",
    "                noise = tf.random.normal([batch_size, latent_dim])\n",
    "                with tf.GradientTape() as gt:\n",
    "                    fake = tgan.generator(noise, training=True)\n",
    "                    loss_g = tgan.generator_loss(tgan.discriminator(fake, training=True))\n",
    "                grads = gt.gradient(loss_g, tgan.generator.trainable_variables)\n",
    "                tgan.gen_optimizer.apply_gradients(zip(grads, tgan.generator.trainable_variables))\n",
    "            \n",
    "            # Discriminator update (×1)\n",
    "            noise = tf.random.normal([batch_size, latent_dim])\n",
    "            with tf.GradientTape() as dt:\n",
    "                fake = tgan.generator(noise, training=True)\n",
    "                real_out = tgan.discriminator(real, training=True)\n",
    "                fake_out = tgan.discriminator(fake, training=True)\n",
    "                loss_d = tgan.discriminator_loss(real_out, fake_out)\n",
    "            grads = dt.gradient(loss_d, tgan.discriminator.trainable_variables)\n",
    "            tgan.disc_optimizer.apply_gradients(zip(grads, tgan.discriminator.trainable_variables))\n",
    "            \n",
    "            batch_g.append(loss_g.numpy())\n",
    "            batch_d.append(loss_d.numpy())\n",
    "        \n",
    "        # Record average losses\n",
    "        avg_g, avg_d = np.mean(batch_g), np.mean(batch_d)\n",
    "        gen_losses.append(avg_g)\n",
    "        disc_losses.append(avg_d)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} – gen_loss={avg_g:.4f}, disc_loss={avg_d:.4f}\")\n",
    "        \n",
    "        # PSNR/SSIM logging\n",
    "        if val_frames.size and (epoch+1) % val_interval == 0:\n",
    "            noise = tf.random.normal([len(val_frames), latent_dim])\n",
    "            gen_val = tgan.generator(noise, training=False).numpy()\n",
    "            real_uint = ((val_frames + 1) * 127.5).astype(np.uint8)\n",
    "            fake_uint = ((gen_val     + 1) * 127.5).astype(np.uint8)\n",
    "            \n",
    "            ps = [peak_signal_noise_ratio(r, f, data_range=255)\n",
    "                  for r, f in zip(real_uint, fake_uint)]\n",
    "            ss = [structural_similarity(r, f, data_range=255, channel_axis=-1)\n",
    "                  for r, f in zip(real_uint, fake_uint)]\n",
    "            \n",
    "            ep_psnr, ep_ssim = np.mean(ps), np.mean(ss)\n",
    "            psnr_scores.append(ep_psnr)\n",
    "            ssim_scores.append(ep_ssim)\n",
    "            print(f\" → val PSNR={ep_psnr:.2f}, SSIM={ep_ssim:.4f}\")\n",
    "        else:\n",
    "            print(\" → Skipping PSNR/SSIM this epoch.\")\n",
    "        \n",
    "        # **New**: Real vs. Generated frame grids\n",
    "        if (epoch+1) % sample_interval == 0:\n",
    "            # pick first 4 validation frames\n",
    "            real_sel = val_frames[:4]\n",
    "            noise = tf.random.normal([4, latent_dim])\n",
    "            fake_sel = tgan.generator(noise, training=False).numpy()\n",
    "            \n",
    "            real_disp = ((real_sel + 1) * 127.5).astype(np.uint8)\n",
    "            fake_disp = ((fake_sel + 1) * 127.5).astype(np.uint8)\n",
    "            \n",
    "            fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "            for i in range(4):\n",
    "                axes[0, i].imshow(real_disp[i])\n",
    "                axes[0, i].axis('off')\n",
    "                axes[1, i].imshow(fake_disp[i])\n",
    "                axes[1, i].axis('off')\n",
    "            fig.suptitle(f\"Epoch {epoch+1}: Real (top) vs Generated (bottom)\")\n",
    "            fig.tight_layout()\n",
    "            \n",
    "            grid_path = os.path.join(output_dir, f\"epoch{epoch+1}_comparison.png\")\n",
    "            fig.savefig(grid_path)\n",
    "            plt.close(fig)\n",
    "            print(f\" → Saved comparison grid: {grid_path}\")\n",
    "    \n",
    "    # Final plot: losses + metrics\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(gen_losses, label='Generator Loss')\n",
    "    plt.plot(disc_losses, label='Discriminator Loss')\n",
    "    plt.plot(psnr_scores, label='PSNR')\n",
    "    plt.plot(ssim_scores, label='SSIM')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_and_metrics.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    return tgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f30ee853-0062-4343-bade-cb232bc793a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TemporalGAN...\n",
      "Building generator with residual blocks...\n",
      "Generator built.\n",
      "Building simplified discriminator...\n",
      "Discriminator built.\n",
      "TemporalGAN initialized.\n",
      "Epoch 1/50 – gen_loss=1.1724, disc_loss=1.1147\n",
      " → val PSNR=9.34, SSIM=0.0999\n",
      "Epoch 2/50 – gen_loss=1.0756, disc_loss=1.2155\n",
      " → val PSNR=9.04, SSIM=0.1166\n",
      "Epoch 3/50 – gen_loss=1.0808, disc_loss=1.1682\n",
      " → val PSNR=8.48, SSIM=0.1053\n",
      "Epoch 4/50 – gen_loss=1.1456, disc_loss=1.1332\n",
      " → val PSNR=8.99, SSIM=0.0978\n",
      "Epoch 5/50 – gen_loss=1.2530, disc_loss=1.0772\n",
      " → val PSNR=8.41, SSIM=0.0885\n",
      "Epoch 6/50 – gen_loss=1.4029, disc_loss=1.0167\n",
      " → val PSNR=9.37, SSIM=0.1281\n",
      "Epoch 7/50 – gen_loss=1.4912, disc_loss=0.9961\n",
      " → val PSNR=9.31, SSIM=0.1398\n",
      "Epoch 8/50 – gen_loss=1.5512, disc_loss=0.9391\n",
      " → val PSNR=9.42, SSIM=0.1415\n",
      "Epoch 9/50 – gen_loss=1.6183, disc_loss=0.9245\n",
      " → val PSNR=9.24, SSIM=0.1364\n",
      "Epoch 10/50 – gen_loss=1.7699, disc_loss=0.9025\n",
      " → val PSNR=8.64, SSIM=0.1214\n",
      " → Saved comparison grid: exp_with_visuals/epoch10_comparison.png\n",
      "Epoch 11/50 – gen_loss=1.8164, disc_loss=0.8627\n",
      " → val PSNR=9.17, SSIM=0.1346\n",
      "Epoch 12/50 – gen_loss=1.9079, disc_loss=0.8649\n",
      " → val PSNR=8.90, SSIM=0.1186\n",
      "Epoch 13/50 – gen_loss=1.9323, disc_loss=0.8175\n",
      " → val PSNR=9.31, SSIM=0.1468\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_gan \u001b[38;5;241m=\u001b[39m train_temporal_gan_with_visuals(\n\u001b[1;32m      2\u001b[0m     frames\u001b[38;5;241m=\u001b[39mtrain_frames,\n\u001b[1;32m      3\u001b[0m     val_frames\u001b[38;5;241m=\u001b[39mval_frames,\n\u001b[1;32m      4\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      5\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m      6\u001b[0m     latent_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m      7\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_with_visuals\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     val_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      9\u001b[0m     sample_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n",
      "Cell \u001b[0;32mIn[37], line 38\u001b[0m, in \u001b[0;36mtrain_temporal_gan_with_visuals\u001b[0;34m(frames, val_frames, epochs, batch_size, latent_dim, output_dir, val_interval, sample_interval)\u001b[0m\n\u001b[1;32m     36\u001b[0m         fake \u001b[38;5;241m=\u001b[39m tgan\u001b[38;5;241m.\u001b[39mgenerator(noise, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m         loss_g \u001b[38;5;241m=\u001b[39m tgan\u001b[38;5;241m.\u001b[39mgenerator_loss(tgan\u001b[38;5;241m.\u001b[39mdiscriminator(fake, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m---> 38\u001b[0m     grads \u001b[38;5;241m=\u001b[39m gt\u001b[38;5;241m.\u001b[39mgradient(loss_g, tgan\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[1;32m     39\u001b[0m     tgan\u001b[38;5;241m.\u001b[39mgen_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, tgan\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Discriminator update (×1)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/backprop.py:1066\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1061\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1062\u001b[0m           output_gradients))\n\u001b[1;32m   1063\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1064\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1066\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m imperative_grad\u001b[38;5;241m.\u001b[39mimperative_grad(\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape,\n\u001b[1;32m   1068\u001b[0m     flat_targets,\n\u001b[1;32m   1069\u001b[0m     flat_sources,\n\u001b[1;32m   1070\u001b[0m     output_gradients\u001b[38;5;241m=\u001b[39moutput_gradients,\n\u001b[1;32m   1071\u001b[0m     sources_raw\u001b[38;5;241m=\u001b[39mflat_sources_raw,\n\u001b[1;32m   1072\u001b[0m     unconnected_gradients\u001b[38;5;241m=\u001b[39munconnected_gradients)\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1075\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[38;5;241m.\u001b[39m_tape,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str(unconnected_gradients\u001b[38;5;241m.\u001b[39mvalue))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/backprop.py:118\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_control_flow_context\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.GradientTape.gradients() does not support graph control flow \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperations like tf.cond or tf.while at this time. Use tf.gradients() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead. If you need this feature, please file a feature request at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/tensorflow/tensorflow/issues/new\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m     )\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gradient_function\u001b[39m(op_name, attr_tuple, num_inputs, inputs, outputs,\n\u001b[1;32m    119\u001b[0m                        out_grads, skip_input_indices, forward_pass_name_scope):\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the gradient function of the op.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    The gradients with respect to the inputs of the function, as a list.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m   mock_op \u001b[38;5;241m=\u001b[39m _MockOp(attr_tuple, inputs, outputs, op_name, skip_input_indices)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_gan = train_temporal_gan_with_visuals(\n",
    "    frames=train_frames,\n",
    "    val_frames=val_frames,\n",
    "    epochs=50,\n",
    "    batch_size=8,\n",
    "    latent_dim=128,\n",
    "    output_dir=\"exp_with_visuals\",\n",
    "    val_interval=1,\n",
    "    sample_interval=10\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
